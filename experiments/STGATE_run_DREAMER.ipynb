{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cd84c5-896b-43f8-a772-f6e857d11823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8dba00b-3951-429f-abc1-321f7d5b8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torcheeg as teeg\n",
    "from torcheeg.datasets.constants.emotion_recognition.dreamer import DREAMER_ADJACENCY_MATRIX, DREAMER_CHANNEL_LOCATION_DICT\n",
    "from torcheeg.datasets.constants.emotion_recognition.seed import SEED_ADJACENCY_MATRIX, SEED_CHANNEL_LOCATION_DICT\n",
    "from torcheeg.datasets.constants.emotion_recognition.seed_iv import SEED_IV_ADJACENCY_MATRIX, SEED_IV_CHANNEL_LOCATION_DICT\n",
    "from torcheeg.datasets import DREAMERDataset, SEEDDataset, SEEDIVDataset\n",
    "from torcheeg import transforms\n",
    "from torcheeg.transforms.pyg import ToG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de53700-5be2-42c8-8a7a-cef8f261cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "772b373a-fc28-471d-883d-5ee23efe4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "\n",
    "class GatedLinearUnits(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hid_channels=16, kernel_size=2, dilation=1,  groups=4, activate=False):\n",
    "        super(GatedLinearUnits, self).__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.activate = activate\n",
    "\n",
    "        self.conv = weight_norm(nn.Conv2d(in_channels, out_channels,\n",
    "                                          (1, kernel_size), \n",
    "                                          dilation=(1, dilation), bias=True, groups=groups))\n",
    "        nn.init.xavier_uniform_(self.conv.weight, gain=np.sqrt(2.0))\n",
    "        nn.init.constant_(self.conv.bias, 0.1)\n",
    "        self.gate = weight_norm(nn.Conv2d(in_channels, out_channels,\n",
    "                                          (1, kernel_size),\n",
    "                                          dilation=(1, dilation), bias=True, groups=groups))\n",
    "        nn.init.xavier_uniform_(self.gate.weight, gain=np.sqrt(2.0))\n",
    "        nn.init.constant_(self.gate.bias, 0.1)\n",
    "        self.downsample = weight_norm(nn.Conv2d(in_channels, out_channels, (1, 1), bias=True))\n",
    "        nn.init.xavier_uniform_(self.downsample.weight, gain=np.sqrt(2.0))\n",
    "        nn.init.constant_(self.downsample.bias, 0.1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(out_channels, momentum=0.2)\n",
    "        self.bn.weight.data.fill_(1)\n",
    "        self.bn.bias.data.fill_(0.1)\n",
    "\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        res = X\n",
    "        gate = X\n",
    "        print('X_in', X.shape)\n",
    "        # X = nn.functional.pad(X, ((self.kernel_size-1)*self.dilation, 0, 0))\n",
    "        out = self.conv(X)\n",
    "        if self.activate:\n",
    "            out = F.tanh(out)\n",
    "\n",
    "        gate = nn.functional.pad(gate, ((self.kernel_size-1)*self.dilation, 0, 0))\n",
    "        gate = self.gate(gate)\n",
    "        gate = self.sigmod(gate)\n",
    "\n",
    "        out = torch.mul(out, gate)\n",
    "        ones = torch.ones_like(gate)\n",
    "\n",
    "        # print('res', res.shape, out.shape)\n",
    "        if res.shape[1] != out.shape[1]:\n",
    "            res = self.downsample(res)\n",
    "        res = torch.mul(res, ones-gate)\n",
    "        out = out + res\n",
    "        # out = self.bn(self.relu(out))\n",
    "        out = self.relu(self.bn(out))\n",
    "        # print('X_out', out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TimeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network block that applies a temporal convolution to each node of\n",
    "    a graph in isolation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, nhid_channels=128, dropout=0.6, layer=3):\n",
    "        \"\"\"\n",
    "        :param in_channels: Number of input features at each node in each time\n",
    "        step.\n",
    "        :param out_channels: Desired number of output channels at each node in\n",
    "        each time step.\n",
    "        :param kernel_size: Size of the 1D temporal kernel.\n",
    "        \"\"\"\n",
    "        super(TimeBlock, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(layer):\n",
    "            print('in_channels', in_channels)\n",
    "            if i == 0:\n",
    "                layers.append(GatedLinearUnits(in_channels, nhid_channels, kernel_size=1, dilation=1, groups=1))\n",
    "                print(i, in_channels, nhid_channels)\n",
    "            elif i == layer-1:\n",
    "                layers.append(GatedLinearUnits(nhid_channels, out_channels, kernel_size=1, dilation=1, groups=1))\n",
    "                print(i, nhid_channels, out_channels)\n",
    "            else:\n",
    "                layers.append(GatedLinearUnits(nhid_channels, nhid_channels, kernel_size=kernel_size, dilation=2**(i), groups=1))\n",
    "                print(i, nhid_channels, nhid_channels)\n",
    "                \n",
    "        self.units = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        print('in', X.shape)\n",
    "        X = X.permute(0, 2, 1)\n",
    "        out = self.units(X)\n",
    "        out = out.permute(0, 1, 2)\n",
    "        print('out', X.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c290bba0-9277-4b34-a0f1-12cc72e15c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_init(conv):\n",
    "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
    "    # nn.init.constant_(conv.bias, 0)\n",
    "\n",
    "\n",
    "def bn_init(bn, scale):\n",
    "    nn.init.constant_(bn.weight, scale)\n",
    "    nn.init.constant_(bn.bias, 0)\n",
    "\n",
    "\n",
    "def fc_init(fc):\n",
    "    nn.init.xavier_normal_(fc.weight)\n",
    "    nn.init.constant_(fc.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b7fb8850-954d-4fc3-b507-15ca804e00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_nodes, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=np.sqrt(2.0))\n",
    "        self.a1 = nn.Parameter(torch.zeros(size=(out_features, 1)))\n",
    "        self.a2 = nn.Parameter(torch.zeros(size=(out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a1.data, gain=np.sqrt(2.0))\n",
    "        nn.init.xavier_uniform_(self.a2.data, gain=np.sqrt(2.0))\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.downsample = nn.Conv1d(in_features, out_features, 1)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(num_nodes, out_features))\n",
    "\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        batch_size = input.size(0)\n",
    "        h = torch.bmm(input, self.W.expand(batch_size, self.in_features, self.out_features))\n",
    "        f_1 = torch.bmm(h, self.a1.expand(batch_size, self.out_features, 1))\n",
    "        f_2 = torch.bmm(h, self.a2.expand(batch_size, self.out_features, 1))\n",
    "        e = self.leakyrelu(f_1 + f_2.transpose(2,1))\n",
    "        # add by xyk\n",
    "        attention = torch.mul(adj, e)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, h) + self.bias.expand(batch_size, self.num_nodes, self.out_features)\n",
    "        if input.shape[-1] != h_prime.shape[-1]:\n",
    "            input = self.downsample(input.permute(0, 2, 1)).permute(0, 2, 1).contiguous()\n",
    "            h_prime = h_prime + input\n",
    "        else:\n",
    "            h_prime = h_prime + input\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class STGATBlock(nn.Module):\n",
    "    def __init__(self, in_channels, spatial_channels, out_channels,\n",
    "                 num_nodes, num_timesteps_input,\n",
    "                 dropout=0.6, alpha=0.2, nheads=4, concat=True):\n",
    "        super(STGATBlock, self).__init__()\n",
    "        self.nheads = nheads\n",
    "        self.concat = concat\n",
    "        self.spatial_channels = spatial_channels\n",
    "        self.temporal1 = nn.Sequential(TimeBlock(in_channels=in_channels,\n",
    "                                                 out_channels=out_channels),\n",
    "        )\n",
    "        self.attentions = [GraphAttentionLayer(\n",
    "            out_channels*(num_timesteps_input),\n",
    "            spatial_channels, num_nodes=num_nodes,\n",
    "            dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm2d(num_nodes)\n",
    "\n",
    "    \n",
    "    def forward(self, X, A_hat):\n",
    "        residual = X\n",
    "        t = self.temporal1(X)\n",
    "        t = t.contiguous().view(t.shape[0], t.shape[1], -1)\n",
    "        if self.concat:\n",
    "            t2 = torch.cat([att(t, A_hat) for att in self.attentions], dim=2)\n",
    "        else:\n",
    "            t2 = sum([att(t, A_hat) for att in self.attentions]) / self.nheads\n",
    "\n",
    "        t2 = t2.view(t2.shape[0], t2.shape[1], -1, self.spatial_channels)\n",
    "        t3 = t2\n",
    "        if t3.shape[-1] == residual.shape[-1]:\n",
    "            t3 = t3 + residual[:,:,-t3.shape[2]:,:]\n",
    "        else:\n",
    "            t3 = t3\n",
    "        return self.relu(self.batch_norm(t3))\n",
    "\n",
    "\n",
    "class DSTANet(nn.Module):\n",
    "    def __init__(self, num_class=60, num_point=128, num_frame=32, num_subset=3, dropout=0., config=None, num_person=2,\n",
    "                 num_channel=64, glo_reg_s=True, att_s=True, glo_reg_t=False, att_t=True,\n",
    "                 use_temporal_att=True, use_spatial_att=True, attentiondrop=0, dropout2d=0, use_pet=True, use_pes=True):\n",
    "        super(DSTANet, self).__init__()\n",
    "\n",
    "        self.out_channels = config[-1][1]\n",
    "        in_channels = config[0][0]\n",
    "\n",
    "        self.input_map = nn.Sequential(\n",
    "            nn.Conv2d(num_channel, in_channels//4, 1),\n",
    "            nn.BatchNorm2d(in_channels//4),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.diff_map1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels//4, in_channels//2, 3),\n",
    "            nn.BatchNorm1d(in_channels//2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.diff_map2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels//2, in_channels, 3),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "        \n",
    "        # param = {\n",
    "        #     'num_node': num_point,\n",
    "        #     'num_subset': num_subset,\n",
    "        #     'glo_reg_s': glo_reg_s,\n",
    "        #     'att_s': att_s,\n",
    "        #     'glo_reg_t': glo_reg_t,\n",
    "        #     'att_t': att_t,\n",
    "        #     'use_spatial_att': use_spatial_att,\n",
    "        #     'use_temporal_att': use_temporal_att,\n",
    "        #     'use_pet': use_pet,\n",
    "        #     'use_pes': use_pes,\n",
    "        #     'attentiondrop': attentiondrop\n",
    "        # }\n",
    "        \n",
    "        self.transformer_block = STGATBlock(in_channels=in_channels, out_channels=self.out_channels,\n",
    "                                            concat=False,\n",
    "                                            spatial_channels=self.out_channels,\n",
    "                                            num_nodes=num_channel, \n",
    "                                            num_timesteps_input=num_point-4, nheads=4)\n",
    "        # self.graph_layers = nn.ModuleList()\n",
    "        # for index, (in_channels, out_channels, inter_channels, stride) in enumerate(config):\n",
    "        #     self.graph_layers.append(\n",
    "        #         STAttentionBlock(in_channels, out_channels, inter_channels, stride=stride, num_frame=num_frame,\n",
    "        #                          **param))\n",
    "        #     num_frame = int(num_frame / stride + 0.5)\n",
    "\n",
    "        self.fc = nn.Linear(self.out_channels, num_class)\n",
    "\n",
    "        # self.drop_out = nn.Dropout(dropout)\n",
    "        # self.drop_out2d = nn.Dropout2d(dropout2d)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                conv_init(m)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                bn_init(m, 1)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                fc_init(m)\n",
    "        \n",
    "\n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: N x C x T\n",
    "                N - batch size, C - channels amount, T - timestamps amount\n",
    "        :return: classes scores\n",
    "        \"\"\"\n",
    "        N, C, T = x.shape\n",
    "\n",
    "        # Features reducing part\n",
    "        x = self.input_map(x.view(N, C, 1, T))\n",
    "        \n",
    "        x = self.diff_map1(x.view(N, -1, T))\n",
    "        x = self.diff_map2(x)\n",
    "        print(x.shape)\n",
    "        x = self.transformer_block(x, A)\n",
    "        \n",
    "\n",
    "        # for i, m in enumerate(self.graph_layers):\n",
    "        #     x = m(x)\n",
    "\n",
    "        # # NM, C, T, V\n",
    "        # x = x.view(N, M, self.out_channels, -1)\n",
    "        # x = x.permute(0, 1, 3, 2).contiguous().view(N, -1, self.out_channels, 1)  # whole channels of one spatial\n",
    "        # x = self.drop_out2d(x)\n",
    "        # x = x.mean(3).mean(1)\n",
    "\n",
    "        # x = self.drop_out(x)  # whole spatial of one channel\n",
    "\n",
    "        # return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e05ee6f-6ac3-4155-a438-4101cb8caea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"..\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71b51ea-0999-4ee9-8f6c-69782b7706d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset already exists at path ..\\data\\dreamer, reading from path...\n"
     ]
    }
   ],
   "source": [
    "dataset = DREAMERDataset(io_path=os.path.join(DATA_DIR, \"dreamer\"),\n",
    "                         mat_path=os.path.join(DATA_DIR, 'DREAMER.mat'),\n",
    "                         online_transform=transforms.Compose([\n",
    "                             ToG(DREAMER_ADJACENCY_MATRIX)\n",
    "                         ]),\n",
    "                         label_transform=transforms.Compose([\n",
    "                             transforms.Select('arousal'),\n",
    "                             transforms.Binary(3.0)\n",
    "                         ]),\n",
    "                         # num_worker=4\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77f00f83-74fb-4db6-b25d-c3da1d59f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheeg.model_selection import KFoldPerSubject\n",
    "\n",
    "SPLIT_PATH = os.path.join(DATA_DIR, 'dreamer_split')\n",
    "k_fold = KFoldPerSubject(n_splits=5,\n",
    "                         split_path=SPLIT_PATH,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aba6b659-926a-4c32-b826-7d720a837485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7e0b2028-d753-4676-9b5f-e8f8bceefecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 64\n",
      "0 64 128\n",
      "in_channels 64\n",
      "1 128 128\n",
      "in_channels 64\n",
      "2 128 2\n",
      "torch.Size([16, 64, 124])\n",
      "in torch.Size([16, 64, 124])\n",
      "X_in torch.Size([16, 124, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 64, 1, 1], expected input[1, 16, 124, 64] to have 64 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 41\u001b[0m\n\u001b[0;32m     36\u001b[0m     A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39medge_index,\n\u001b[0;32m     37\u001b[0m                                 batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39medge_weight,\n\u001b[0;32m     38\u001b[0m                                 size\u001b[38;5;241m=\u001b[39m(batch_size, n_channels\u001b[38;5;241m*\u001b[39mn_channels)\n\u001b[0;32m     39\u001b[0m                                )\u001b[38;5;241m.\u001b[39mto_dense()\u001b[38;5;241m.\u001b[39mview(batch_size, n_channels, n_channels)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# A = batch[0].edge_index.view(batch_size, 2, -1)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[114], line 172\u001b[0m, in \u001b[0;36mDSTANet.forward\u001b[1;34m(self, x, A)\u001b[0m\n\u001b[0;32m    170\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiff_map2(x)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 172\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[114], line 75\u001b[0m, in \u001b[0;36mSTGATBlock.forward\u001b[1;34m(self, X, A_hat)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, A_hat):\n\u001b[0;32m     74\u001b[0m     residual \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m---> 75\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[118], line 98\u001b[0m, in \u001b[0;36mTimeBlock.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     97\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[118], line 43\u001b[0m, in \u001b[0;36mGatedLinearUnits.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_in\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# X = nn.functional.pad(X, ((self.kernel_size-1)*self.dilation, 0, 0))\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivate:\n\u001b[0;32m     45\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mtanh(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 64, 1, 1], expected input[1, 16, 124, 64] to have 64 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# from torcheeg.models import CCNN\n",
    "\n",
    "from torcheeg.model_selection import train_test_split\n",
    "# from torch.utils.data.dataloader import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 16\n",
    "n_channels = 14\n",
    "\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "\n",
    "for i, (train_dataset, test_dataset) in enumerate(k_fold.split(dataset)):\n",
    "    # initialize model\n",
    "    # model = CCNN(num_classes=2, in_channels=4, grid_size=(9, 9)).to(device)\n",
    "    model = DSTANet(num_class=2, num_channel=n_channels, config=[[64, 2], [2, 2]]).to(device)\n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=1e-4)  # official: weight_decay=5e-1\n",
    "    # split train and val\n",
    "    train_dataset, val_dataset = train_test_split(\n",
    "        train_dataset,\n",
    "        test_size=0.2,\n",
    "        split_path=SPLIT_PATH+f'{i}',\n",
    "        shuffle=True)\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        X = batch[0].x.view(batch_size, n_channels, -1).to(device)\n",
    "        y = batch[1].to(device)\n",
    "        A = torch.sparse_coo_tensor(batch[0].edge_index,\n",
    "                                    batch[0].edge_weight,\n",
    "                                    size=(batch_size, n_channels*n_channels)\n",
    "                                   ).to_dense().view(batch_size, n_channels, n_channels)\n",
    "        # A = batch[0].edge_index.view(batch_size, 2, -1)\n",
    "        model(X, A)\n",
    "\n",
    "        \n",
    "        break\n",
    "    break\n",
    "        # print(batch[0].shape)\n",
    "        # print(np.unique(batch[1]))\n",
    "        # X = batch[0].to(device)\n",
    "        # y = batch[1].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50280ade-b682-4857-9bcd-17ba11b12ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7115476c-b673-45f1-a615-b22dfd43fb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
